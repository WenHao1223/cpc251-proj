{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction (Read this)\n",
    "- Use this template to develop your project. Do not change the steps. \n",
    "- For each step, you may add additional cells if needed.\n",
    "- But remove <b>unnecessary</b> cells to ensure the notebook is readable.\n",
    "- Marks will be <b>deducted</b> if the notebook is cluttered or difficult to follow due to excess or irrelevant content.\n",
    "- <b>Briefly</b> describe the steps in the \"Description:\" field.\n",
    "- <b>Do not</b> submit the dataset. \n",
    "- The submitted jupyter notebook will be executed using the uploaded dataset in eLearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Information\n",
    "\n",
    "Group No: Seismic 6\n",
    "\n",
    "- Member 1: Lim Wen Hao\n",
    "- Member 2: Goey Chew Hong\n",
    "- Member 3: Lim Cong Sheng\n",
    "- Member 4: Tan Jun Cheng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False # comment if not needed\n",
    "\n",
    "# Import necessary libraries for data manipulation and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import necessary libraries for machine learning\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier # for KNN\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_text # for decision tree\n",
    "\n",
    "# Import necessary libraries for Feature Selection and Sampling\n",
    "from imblearn.over_sampling import SMOTE # for oversampling\n",
    "from collections import Counter # for counting\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset README documentation, \n",
    "\n",
    "**Attribute information:**\n",
    "1. `seismic`: result of shift seismic hazard assessment in the mine working obtained by the seismic method (a - lack of hazard, b - low hazard, c - high hazard, d - danger state);\n",
    "2. `seismoacoustic`: result of shift seismic hazard assessment in the mine working obtained by the seismoacoustic method;\n",
    "3. `shift`: information about type of a shift (W - coal-getting, N -preparation shift);\n",
    "4. `genergy`: seismic energy recorded within previous shift by the most active geophone (GMax) out of geophones monitoring the longwall;\n",
    "5. `gpuls`: a number of pulses recorded within previous shift by GMax;\n",
    "6. `gdenergy`: a deviation of energy recorded within previous shift by GMax from average energy recorded during eight previous shifts;\n",
    "7. `gdpuls`: a deviation of a number of pulses recorded within previous shift by GMax from average number of pulses recorded during eight previous shifts;\n",
    "8. `ghazard`: result of shift seismic hazard assessment in the mine working obtained by the seismoacoustic method based on registration coming form GMax only;\n",
    "9. `nbumps`: the number of seismic bumps recorded within previous shift;\n",
    "10. `nbumps2`: the number of seismic bumps (in energy range [10^2,10^3)) registered within previous shift;\n",
    "11. `nbumps3`: the number of seismic bumps (in energy range [10^3,10^4)) registered within previous shift;\n",
    "12. `nbumps4`: the number of seismic bumps (in energy range [10^4,10^5)) registered within previous shift;\n",
    "13. `nbumps5`: the number of seismic bumps (in energy range [10^5,10^6)) registered within the last shift;\n",
    "14. `nbumps6`: the number of seismic bumps (in energy range [10^6,10^7)) registered within previous shift;\n",
    "15. `nbumps7`: the number of seismic bumps (in energy range [10^7,10^8)) registered within previous shift;\n",
    "16. `nbumps89`: the number of seismic bumps (in energy range [10^8,10^10)) registered within previous shift;\n",
    "17. `energy`: total energy of seismic bumps registered within previous shift;\n",
    "18. `maxenergy`: the maximum energy of the seismic bumps registered within previous shift;\n",
    "19. `class`: the decision attribute - \"1\" means that high energy seismic bump occurred in the next shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"seismic-bumps.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the sample we got, `class` column consists only 0 and 1 which is the target label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary we get,\n",
    "1. No null values found in all columns\n",
    "2. There are 19 columns and 2584 records found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target variable\n",
    "label = \"class\"\n",
    "df[label].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the column based on data types\n",
    "categorical_df = df.select_dtypes(include='object')\n",
    "categorical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the column based on data types\n",
    "numerical_df = df.select_dtypes(include='int64')\n",
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the value counts, 2414 records found with `class` = 0, and 170 records of `class` = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation matrix for numerical features\n",
    "corr = numerical_df.corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the correlation matrix, among the features that highly correlated with each others include:\n",
    "- `energy` and `maxenergy` (0.99)\n",
    "- `gdpuls` and `gdenergy` (0.81)\n",
    "- `nbumps5` and `maxenergy` (0.81)\n",
    "- `nbumps` and `nbumps2` (0.8)\n",
    "- `nbumps` and `nbumps3` (0.8)\n",
    "- `nbumps5` and `energy` (0.77)\n",
    "- `gpuls` and `genergy` (0.75)\n",
    "\n",
    "Features that do not show any correlation with other classes:\n",
    "- `nbumps6`\n",
    "- `nbumps7`\n",
    "- `nbumps89`\n",
    "\n",
    "These features can be dropped in the subsequent steps as they do not need to be involved in the model training.\n",
    "\n",
    "**None of the features** show direct correlation with the target column, `class`.\n",
    "\n",
    "`class` column can be dropped from numerical DataFrame `numerical_df` as it is a target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features that are not needed for the analysis\n",
    "df.drop(['nbumps89', 'nbumps7', 'nbumps6'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the features that are not needed for the analysis and the target variable\n",
    "numerical_df.drop(['nbumps89', 'nbumps7', 'nbumps6', 'class'], axis=1, inplace=True)\n",
    "numerical_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the numerical features\n",
    "numerical_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize the continuous features\n",
    "continuous_df = numerical_df[[\"genergy\", \"gpuls\", \"gdenergy\", \"gdpuls\", \"energy\", \"maxenergy\"]]\n",
    "continuous_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of continuous features\n",
    "sns.pairplot(continuous_df, size = 2.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Loop through the columns and create subplots\n",
    "for i, column in enumerate(continuous_df.columns, 1):\n",
    "  plt.subplot(2, 3, i)  # Create a subplot in a 2x3 grid\n",
    "  fig = df.boxplot(column=column)\n",
    "  fig.set_title('')\n",
    "  fig.set_ylabel(column)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the duplicated rows\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reindex the DataFrame after dropping duplicates\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for the dimensions of the DataFrame after data cleaning\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame\n",
    "X = df.drop(columns=label).values\n",
    "y = df[label].values\n",
    "\n",
    "# Show the shape of the data\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset\n",
    "Split the dataset into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "# Training : Validation : Test = 7 : 1 : 2\n",
    "\n",
    "seed_num = 42\n",
    "np.random.seed(seed_num)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "# test_size = 0.2 as (training dataset + validation dataset) : test dataset = 8 : 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_num,\n",
    "  # stratify=y\n",
    ")\n",
    "\n",
    "# Split the training dataset into training and validation sets\n",
    "# test_size = 0.125 as training dataset : validation dataset = 7 : 1\n",
    "X_train, X_vald, y_train, y_vald = train_test_split(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  test_size=0.125,\n",
    "  random_state=seed_num,\n",
    "  # stratify=y_train\n",
    ")\n",
    "\n",
    "# Show the shape of the datasets\n",
    "print(\"Original dataset shape:\")\n",
    "print(\"X shape:\", X.shape, \", y shape:\", y.shape, \"\\n\")\n",
    "\n",
    "print(\"Training set shape:\")\n",
    "print(\"X_train shape:\", X_train.shape, \", y_train shape:\", y_train.shape, \"\\n\")\n",
    "\n",
    "print(\"Validation set shape:\")\n",
    "print(\"X_vald shape:\", X_vald.shape, \", y_vald shape:\", y_vald.shape, \"\\n\")\n",
    "\n",
    "print(\"Test set shape: \")\n",
    "print(\"X_test shape:\", X_test.shape, \", y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing\n",
    "Perform data preprocessing such as normalization, standardization, label encoding etc.\n",
    "______________________________________________________________________________________\n",
    "Description:\n",
    "\n",
    "This section outlines a comprehensive data preprocessing pipeline designed to prepare the Seismic Bumps Dataset for a classification task aimed at predicting whether a high-energy seismic bump will occur in the next shift.\n",
    "\n",
    "The process includes **label encoding and data scaling**: ordinal categorical features are transformed using `Label Encoding,` while nominal features are encoded with `One-Hot Encoding`. Numerical features are scaled using the `Robust Scaler` to reduce the impact of outliers and skewed distributions. To address class imbalance, the `SMOTE (Synthetic Minority Over-sampling Technique)` is applied exclusively to the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Label encoding & data scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the column description, DataFrame consists of\n",
    "\n",
    "Categorical data includes\n",
    "- Ordinal: `seismic`, `seismoacoustic`, and `ghazard`\n",
    "- Nominal: `shift`\n",
    "\n",
    "and numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are encoded based on their categories:\n",
    "- Ordinal categorical values using **Label Encoding**,\n",
    "- Nominal categorical values using **One Hot Encoding**,\n",
    "- and Numerical values using **Robust Scaler**\n",
    "\n",
    "**Robust Scaler** is used instead of **MinMaxScaler** as seismic data often includes extreme value (e.g. sudden energy spikes, zero-heavy counts). RoubstScaler is typically safer as it is good for data that has outliers or heavy skew."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train to a DataFrame for scaling\n",
    "X_train_df = pd.DataFrame(X_train, columns=df.drop(columns=label).columns)\n",
    "X_vald_df = pd.DataFrame(X_vald, columns=df.drop(columns=label).columns)\n",
    "X_test_df = pd.DataFrame(X_test, columns=df.drop(columns=label).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Label Encoding to the ordinal categorical columns\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in categorical_df.columns:\n",
    "  if column != 'shift':\n",
    "    # Transform the training, validation, and test sets\n",
    "    X_train_df[column] = label_encoder.fit_transform(X_train_df[column])\n",
    "    X_vald_df[column] = label_encoder.transform(X_vald_df[column])\n",
    "    X_test_df[column] = label_encoder.transform(X_test_df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply One-Hot Encoding to the nominal categorical columns\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "def onehot_encode_and_concat(df, column, onehot_encoder): \n",
    "  onehot_encoded = onehot_encoder.transform(df[[column]])\n",
    "  onehot_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out([column]), index=df.index)\n",
    "  df = df.drop(column, axis=1) # drop the original column\n",
    "  return pd.concat([df, onehot_df], axis=1)\n",
    "\n",
    "onehot_encoder.fit(X_train_df[['shift']])\n",
    "\n",
    "# Transform the training, validation, and test sets\n",
    "X_train_df = onehot_encode_and_concat(X_train_df, 'shift', onehot_encoder)\n",
    "X_vald_df = onehot_encode_and_concat(X_vald_df, 'shift', onehot_encoder)\n",
    "X_test_df = onehot_encode_and_concat(X_test_df, 'shift', onehot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Robust Scaling to the numerical columns\n",
    "numerical_scaler = RobustScaler()\n",
    "\n",
    "# Transform the training, validation, and test sets\n",
    "X_train_df[numerical_df.columns] = numerical_scaler.fit_transform(X_train_df[numerical_df.columns])\n",
    "X_vald_df[numerical_df.columns] = numerical_scaler.transform(X_vald_df[numerical_df.columns])\n",
    "X_test_df[numerical_df.columns] = numerical_scaler.transform(X_test_df[numerical_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint the columns after encoding\n",
    "columns_after_encoding = X_train_df.columns\n",
    "columns_after_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the scaled columns of the training set\n",
    "X_train_df[numerical_df.columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the scaled columns of the validation set\n",
    "X_vald_df[numerical_df.columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the scaled columns of the test set\n",
    "X_test_df[numerical_df.columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Oversampling\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)**, is used for oversampling the minority class in imbalanced datasets. It is only introduced to training dataset to prevent the unrealistic samples and data leakage in validation dataset and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampling\n",
    "# SMOTE\n",
    "smote = SMOTE(random_state=seed_num, sampling_strategy='minority')\n",
    "X_smote, y_smote = smote.fit_resample(X_train_df, y_train)\n",
    "\n",
    "print('Original dataset shape:', Counter(y_train))\n",
    "print('Resample dataset shape:', Counter(y_smote))\n",
    "\n",
    "X_train = X_smote\n",
    "y_train = y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to a NumPy array\n",
    "X_vald = X_vald_df.values\n",
    "X_test = X_test_df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Selection\n",
    "Perform feature selection to select the relevant features.\n",
    "______________________________________________________________________________________\n",
    "Description:\n",
    "\n",
    "Feature selection is then performed using the `SelectKBest` method with mutual information to retain the eight most informative variables.\n",
    "\n",
    "The final output consists of clean, balanced, and scaled datasets, ready for model development and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "selector = SelectKBest(score_func=mutual_info_classif, k=8)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "# Transform the validation and test sets using the same selector\n",
    "X_vald = selector.transform(X_vald)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# Get the feature names after encoding\n",
    "selected_indices = selector.get_support(indices=True) # Get the indices of the selected features\n",
    "selected_features = columns_after_encoding[selected_indices] # Get the names of the selected features\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all feature names BEFORE selection\n",
    "all_feature_names = X_train_df.columns  # assuming x_train is a DataFrame\n",
    "\n",
    "# Get all feature scores from the selector\n",
    "all_scores = selector.scores_  # same length as original feature count\n",
    "\n",
    "# Create a DataFrame of all features and their scores\n",
    "feature_importances = pd.DataFrame({\n",
    "  \"feature\": all_feature_names,\n",
    "  \"importance\": all_scores\n",
    "})\n",
    "\n",
    "# Sort the DataFrame by importance\n",
    "feature_importances = feature_importances.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Plot all features\n",
    "plt.figure(figsize=(12, max(6, len(feature_importances) * 0.3)))  # dynamic height\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances)\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (All Features)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "# Get the indices of the selected features\n",
    "importances = selector.scores_[selected_indices] # Get the scores of the selected features\n",
    "feature_importances = pd.DataFrame({\"feature\": selected_features, \"importance\": importances}) # Create a DataFrame with feature names and their importances\n",
    "feature_importances = feature_importances.sort_values(\"importance\", ascending=False) # Sort the DataFrame by importance\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x=feature_importances['importance'], y=feature_importances['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance (Selected Features)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the selected features back into the training set\n",
    "X_train_df = pd.DataFrame(X_train, columns=selected_features)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the selected features back into the validation set\n",
    "X_vald_df = pd.DataFrame(X_vald, columns=selected_features)\n",
    "X_vald_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the selected features back into the test set\n",
    "X_test_df = pd.DataFrame(X_test, columns=selected_features)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to a NumPy array\n",
    "X_train = X_train_df.values\n",
    "X_vald = X_vald_df.values\n",
    "X_test = X_test_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the datasets after oversampling\n",
    "print(\"Training set shape:\")\n",
    "print(\"X_train shape:\", X_train.shape, \", y_train shape:\", y_train.shape)\n",
    "print(\"X_train_df shape:\", X_train_df.shape, \", y_train shape:\", y_train.shape, \"\\n\")\n",
    "\n",
    "print(\"Validation set shape:\")\n",
    "print(\"X_vald shape:\", X_vald.shape, \", y_vald shape:\", y_vald.shape)\n",
    "print(\"X_vald_df shape:\", X_vald_df.shape, \", y_vald shape:\", y_vald.shape, \"\\n\")\n",
    "\n",
    "print(\"Test set shape: \")\n",
    "print(\"X_test shape:\", X_test.shape, \", y_test shape:\", y_test.shape)\n",
    "print(\"X_test_df shape:\", X_test_df.shape, \", y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data modeling\n",
    "Build the machine learning models. You must build atleast two (2) predictive models. One of the predictive models must be either Decision Tree or Support Vector Machine.\n",
    "______________________________________________________________________________________\n",
    "Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Variables & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeated Stratified K-Fold Cross Validation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices_side_by_side(y_true, y_pred_default, y_pred_optimized, model_name):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices for two models side by side\n",
    "    \"\"\"\n",
    "    cm_default = confusion_matrix(y_true, y_pred_default)\n",
    "    cm_optimized = confusion_matrix(y_true, y_pred_optimized)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Default model\n",
    "    sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix - Default {model_name}')\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "\n",
    "    # Optimized model\n",
    "    sns.heatmap(cm_optimized, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "    axes[1].set_title(f'Confusion Matrix - Optimized {model_name}')\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    axes[1].set_ylabel('True Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_validation_set(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Evaluate the validation set using various metrics and print the results.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): True labels of the validation set.\n",
    "    y_pred (array-like): Predicted labels of the validation set.\n",
    "    \"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"📊 Evaluation metrics for validation set:\")\n",
    "    print(f\"Validation set accuracy: {accuracy_score(y_true, y_pred):.6f}\")\n",
    "    print(f\"Validation set precision: {precision_score(y_true, y_pred):.6f}\")\n",
    "    print(f\"Validation set recall: {recall_score(y_true, y_pred):.6f}\")\n",
    "    print(f\"Validation set f1 score: {f1_score(y_true, y_pred):.6f}\")\n",
    "    print(\"\\nValidation set confusion matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    # Print the classification report for the validation set\n",
    "    print(\"\\nValidation set classification report: \\n\", classification_report(y_vald, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, model, model_short_name):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for a model.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true (array-like): True labels of the validation set.\n",
    "    model (object): Trained model.\n",
    "    model_short_name (str): Short name of the model for labeling the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get probability predictions for class 1\n",
    "    y_proba = model.predict_proba(X_vald)[:, 1]\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) Curve - Optimized {model_short_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_comparison(y_vald, y_pred_default, y_pred_optimized, model_name, model_short_name):\n",
    "    \"\"\"\n",
    "    Plot the model comparison for the validation set\n",
    "    \"\"\"\n",
    "    # Prepare data for the bar graph\n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
    "    default_scores = [\n",
    "        accuracy_score(y_vald, y_pred_default),\n",
    "        precision_score(y_vald, y_pred_default),\n",
    "        recall_score(y_vald, y_pred_default),\n",
    "        f1_score(y_vald, y_pred_default)\n",
    "    ]\n",
    "    optimized_scores = [\n",
    "        accuracy_score(y_vald, y_pred_optimized),\n",
    "        precision_score(y_vald, y_pred_optimized),\n",
    "        recall_score(y_vald, y_pred_optimized),\n",
    "        f1_score(y_vald, y_pred_optimized)\n",
    "    ]\n",
    "\n",
    "    # Plot the bar graph\n",
    "    x = np.arange(len(metrics))  # the label locations\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    bars1 = ax.bar(x - width/2, default_scores, width, label='Default ' + model_short_name)\n",
    "    bars2 = ax.bar(x + width/2, optimized_scores, width, label='Optimized ' + model_short_name)\n",
    "\n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_xlabel('Metrics')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Performance Comparison of Default and Optimized ' + model_name)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend()\n",
    "\n",
    "    # Annotate bars with their values\n",
    "    for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.6f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.annotate(f'{height:.6f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of SVM with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Support Vector Machine\n",
    "svm_default_model = SVC()\n",
    "\n",
    "# Fit the Support Vector Machine on the training set\n",
    "svm_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_svm_default_model_pred_val = svm_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the Support Vector Machine\n",
    "print(\"Default parameters of Support Vector Machine: \\n\", svm_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_svm_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of SVM linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM linear model\n",
    "svm_linear_default_model = SVC(kernel='linear', C=1)\n",
    "\n",
    "# Fit the SVM linear model on the training set\n",
    "svm_linear_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_svm_linear_default_model_pred_val = svm_linear_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the SVM linear model\n",
    "print(\"Default parameters of SVM linear model: \\n\", svm_linear_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_svm_linear_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of SVM Radial Basis Function (RBF) kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM RBF model\n",
    "svm_rbf_default_model = SVC(kernel='rbf', C=10, gamma=0.00001)\n",
    "\n",
    "# Fit the SVM RBF model on the training set\n",
    "svm_rbf_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_svm_rbf_default_model_pred_val = svm_rbf_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the SVM RBF model\n",
    "print(\"Default parameters of SVM RBF model: \\n\", svm_rbf_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_svm_rbf_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of SVM sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SVM sigmoid model\n",
    "svm_sigmoid_default_model = SVC(kernel='sigmoid', C=1, gamma=0.1, coef0=1)\n",
    "\n",
    "# Fit the SVM sigmoid model on the training set\n",
    "svm_sigmoid_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_svm_sigmoid_default_model_pred_val = svm_sigmoid_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the SVM sigmoid model\n",
    "print(\"Default parameters of SVM sigmoid model: \\n\", svm_sigmoid_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_svm_sigmoid_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using GridSearchCV for SVM with linear, RBF, sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameter grid for the SVM model\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 5, 10],\n",
    "    'coef0': [0, 0.1, 0.5],\n",
    "    'gamma': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "# Initialize the base SVM model\n",
    "svm = SVC(\n",
    "    probability=True,  # Enable probability estimates for ROC curve\n",
    ")\n",
    "\n",
    "# Set up GridSearchCV\n",
    "svm_grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring=\"recall\", # Choose to refit the base model based on recall score\n",
    "    refit=\"recall\",\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    error_score=0,\n",
    ")\n",
    "\n",
    "# Fit the grid search to training data\n",
    "svm_grid_results = svm_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best hyperparameters and best cross-validation score\n",
    "print(f\"✅ Best hyperparameters found: {svm_grid_results.best_params_}\")\n",
    "print(f\"✅ Best cross-validation score: {svm_grid_results.best_score_:.6f}\")\n",
    "\n",
    "# Get the best model\n",
    "svm_best_model = svm_grid_results.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_svm_best_pred_val = svm_best_model.predict(X_vald)\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_svm_best_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results of the GridSearchCV into a DataFrame\n",
    "# Sort the DataFrame by the mean test score in descending order\n",
    "# Display the top 10 configurations with the highest mean test scores\n",
    "results_svm_df = pd.DataFrame(svm_grid_search.cv_results_)\n",
    "results_svm_df.sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'], \n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'],\n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results of the GridSearchCV into a DataFrame\n",
    "results_svm_df = pd.DataFrame(svm_grid_search.cv_results_)\n",
    "\n",
    "# Set up a 3x2 grid of subplots to include all visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 24))\n",
    "fig.suptitle('SVM Hyperparameter Tuning Results', fontsize=16)\n",
    "\n",
    "# Function to safely create a heatmap, handling empty dataframes\n",
    "def safe_heatmap(filtered_df, x_param, y_param, ax, title):\n",
    "    if filtered_df.empty:\n",
    "        ax.text(0.5, 0.5, f\"No data for\\n{title}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "    \n",
    "    pivot = filtered_df.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index=f'param_{y_param}',\n",
    "        columns=f'param_{x_param}'\n",
    "    )\n",
    "    \n",
    "    if pivot.empty or pivot.size == 0:\n",
    "        ax.text(0.5, 0.5, f\"No data for\\n{title}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "        \n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_param)\n",
    "    ax.set_ylabel(y_param)\n",
    "\n",
    "# 1. C vs gamma (for linear kernel)\n",
    "df_filtered1 = results_svm_df[results_svm_df['param_kernel'] == 'linear']\n",
    "safe_heatmap(\n",
    "    df_filtered1, \n",
    "    'C', \n",
    "    'gamma', \n",
    "    axes[0, 0], \n",
    "    'C vs gamma\\n(kernel=linear)'\n",
    ")\n",
    "\n",
    "# 2. C vs coef0 (for rbf kernel)\n",
    "df_filtered2 = results_svm_df[results_svm_df['param_kernel'] == 'rbf']\n",
    "safe_heatmap(\n",
    "    df_filtered2, \n",
    "    'C', \n",
    "    'coef0', \n",
    "    axes[0, 1], \n",
    "    'C vs coef0\\n(kernel=rbf)'\n",
    ")\n",
    "\n",
    "# 3. gamma vs coef0 (for rbf kernel)\n",
    "df_filtered3 = results_svm_df[results_svm_df['param_kernel'] == 'rbf']\n",
    "safe_heatmap(\n",
    "    df_filtered3, \n",
    "    'gamma', \n",
    "    'coef0', \n",
    "    axes[1, 0], \n",
    "    'gamma vs coef0\\n(kernel=rbf)'\n",
    ")\n",
    "\n",
    "# 4. C vs gamma (for rbf kernel)\n",
    "df_filtered4 = results_svm_df[results_svm_df['param_kernel'] == 'rbf']\n",
    "safe_heatmap(\n",
    "    df_filtered4, \n",
    "    'C', \n",
    "    'gamma', \n",
    "    axes[1, 1], \n",
    "    'C vs gamma\\n(kernel=rbf)'\n",
    ")\n",
    "\n",
    "# 5. C vs kernel\n",
    "df_filtered5 = results_svm_df[results_svm_df['param_gamma'] == 1]\n",
    "safe_heatmap(\n",
    "    df_filtered5, \n",
    "    'C', \n",
    "    'kernel', \n",
    "    axes[2, 0], \n",
    "    'C vs kernel\\n(gamma=1)'\n",
    ")\n",
    "\n",
    "# 6. Bar chart of kernel performance\n",
    "kernel_scores = results_svm_df.groupby('param_kernel')['mean_test_score'].mean()\n",
    "sns.barplot(x=kernel_scores.index, y=kernel_scores.values, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Average Performance by Kernel')\n",
    "axes[2, 1].set_xlabel('Kernel')\n",
    "axes[2, 1].set_ylabel('Mean Test Score')\n",
    "for i, v in enumerate(kernel_scores.values):\n",
    "    axes[2, 1].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for one kernel and one coef0 for simplicity\n",
    "svm_subset = results_svm_df[\n",
    "    (results_svm_df['param_kernel'] == 'rbf') &\n",
    "    (results_svm_df['param_coef0'] == 0.0)\n",
    "]\n",
    "\n",
    "pivot = svm_subset.pivot_table(\n",
    "    index='param_C',\n",
    "    columns='param_gamma',\n",
    "    values='mean_test_score'\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap=\"YlGnBu\")\n",
    "plt.title('Recall Heatmap for RBF Kernel (coef0=0)')\n",
    "plt.ylabel('C')\n",
    "plt.xlabel('Gamma')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track recall for different kernels across C values\n",
    "for svm_kernel in svm_param_grid['kernel']:\n",
    "    svm_subset = results_svm_df[\n",
    "        (results_svm_df['param_kernel'] == svm_kernel) &\n",
    "        (results_svm_df['param_gamma'] == 1) & \n",
    "        (results_svm_df['param_coef0'] == 0.0)\n",
    "    ]\n",
    "    plt.plot(svm_subset['param_C'], svm_subset['mean_test_score'], label=svm_kernel)\n",
    "\n",
    "plt.title(\"Recall vs C for Different Kernels (gamma=1, coef0=0)\")\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"Mean Recall\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for the optimized SVM model\n",
    "plot_roc_curve(y_vald, svm_best_model, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices_side_by_side(y_vald, y_svm_default_model_pred_val, y_svm_best_pred_val, \"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(y_vald, y_svm_default_model_pred_val, y_svm_best_pred_val, \"Support Vector Machine\", \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier (DTC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of DTC with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Decision Tree Classifier\n",
    "dtc_default_model = DecisionTreeClassifier(random_state=seed_num)\n",
    "\n",
    "# Fit the Decision Tree Classifier on the training set\n",
    "dtc_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_dtc_default_model_pred_val = dtc_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the Decision Tree Classifier\n",
    "print(\"Default parameters of Decision Tree Classifier: \\n\", dtc_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_dtc_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using GridSearchCV for DTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameter grid for the Decision Tree Classifier\n",
    "dtc_param_grid = {\n",
    "    'ccp_alpha': [0.0, 0.0005, 0.001],\n",
    "    'class_weight': ['balanced'],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [5, 7, 9, 11, 13],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None],\n",
    "    'min_impurity_decrease': [0.0, 0.001, 0.01],\n",
    "    'min_samples_leaf': [1, 2, 4, 6],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'splitter': ['best'],\n",
    "}\n",
    "\n",
    "# Initialize the base Decision Tree\n",
    "dtc = DecisionTreeClassifier(random_state=seed_num)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "dtc_grid_search = GridSearchCV(\n",
    "    estimator=dtc,\n",
    "    param_grid=dtc_param_grid,\n",
    "    scoring=\"recall\", # Choose to refit the base model based on recall score\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    error_score=0,\n",
    ")\n",
    "\n",
    "# Fit the grid search to training data\n",
    "dtc_grid_results = dtc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best hyperparameters and best cross-validation score\n",
    "print(f\"✅ Best hyperparameters found: {dtc_grid_results.best_params_}\")\n",
    "print(f\"✅ Best cross-validation score: {dtc_grid_results.best_score_:.6f}\")\n",
    "\n",
    "# Get the best model\n",
    "dtc_best_model = dtc_grid_results.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_dtc_best_pred_val = dtc_best_model.predict(X_vald)\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_dtc_best_pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results of the GridSearchCV into a DataFrame\n",
    "# Sort the DataFrame by the mean test score in descending order\n",
    "# Display the top 10 configurations with the highest mean test scores\n",
    "results_dtc_df = pd.DataFrame(dtc_grid_search.cv_results_)\n",
    "results_dtc_df.sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'], \n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dtc_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'],\n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization with multiple heatmaps of DTC hyperparameters tuning results\n",
    "results_dtc_df = pd.DataFrame(dtc_grid_search.cv_results_)\n",
    "\n",
    "# Set up a 3x2 grid of subplots to include all visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 24))\n",
    "fig.suptitle('DTC Hyperparameter Tuning Results', fontsize=16)\n",
    "\n",
    "# Function to safely create a heatmap, handling empty dataframes\n",
    "def safe_heatmap(filtered_df, x_param, y_param, ax, title):\n",
    "    if filtered_df.empty:\n",
    "        ax.text(0.5, 0.5, f\"No data for\\n{title}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "    \n",
    "    pivot = filtered_df.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index=f'param_{y_param}',\n",
    "        columns=f'param_{x_param}'\n",
    "    )\n",
    "    \n",
    "    if pivot.empty or pivot.size == 0:\n",
    "        ax.text(0.5, 0.5, f\"No data for\\n{title}\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "        ax.set_title(title)\n",
    "        return\n",
    "        \n",
    "    sns.heatmap(pivot, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(x_param)\n",
    "    ax.set_ylabel(y_param)\n",
    "\n",
    "# 1. max_depth vs min_samples_leaf\n",
    "df_filtered1 = results_dtc_df[\n",
    "    (results_dtc_df['param_criterion'] == 'gini') &\n",
    "    (results_dtc_df['param_ccp_alpha'] == 0.0005)\n",
    "]\n",
    "safe_heatmap(\n",
    "    df_filtered1, \n",
    "    'min_samples_leaf', \n",
    "    'max_depth', \n",
    "    axes[0, 0], \n",
    "    'max_depth vs min_samples_leaf\\n(criterion=gini, ccp_alpha=0.0005)'\n",
    ")\n",
    "\n",
    "# 2. max_depth vs criterion\n",
    "df_filtered2 = results_dtc_df[\n",
    "    (results_dtc_df['param_min_samples_leaf'] == 2) &\n",
    "    (results_dtc_df['param_ccp_alpha'] == 0.0005)\n",
    "]\n",
    "safe_heatmap(\n",
    "    df_filtered2, \n",
    "    'criterion', \n",
    "    'max_depth', \n",
    "    axes[0, 1], \n",
    "    'max_depth vs criterion\\n(min_samples_leaf=2, ccp_alpha=0.0005)'\n",
    ")\n",
    "\n",
    "# 3. max_depth vs ccp_alpha\n",
    "df_filtered3 = results_dtc_df[\n",
    "    (results_dtc_df['param_criterion'] == 'gini') &\n",
    "    (results_dtc_df['param_min_samples_leaf'] == 2)\n",
    "]\n",
    "safe_heatmap(\n",
    "    df_filtered3, \n",
    "    'ccp_alpha', \n",
    "    'max_depth', \n",
    "    axes[1, 0], \n",
    "    'max_depth vs ccp_alpha\\n(criterion=gini, min_samples_leaf=2)'\n",
    ")\n",
    "\n",
    "# 4. max_depth vs max_features\n",
    "df_filtered4 = results_dtc_df[\n",
    "    (results_dtc_df['param_criterion'] == 'gini') &\n",
    "    (results_dtc_df['param_min_samples_leaf'] == 2) &\n",
    "    (results_dtc_df['param_ccp_alpha'] == 0.0005)\n",
    "]\n",
    "safe_heatmap(\n",
    "    df_filtered4, \n",
    "    'max_features', \n",
    "    'max_depth', \n",
    "    axes[1, 1], \n",
    "    'max_depth vs max_features\\n(criterion=gini, min_samples_leaf=2)'\n",
    ")\n",
    "\n",
    "# 5. max_features vs min_impurity_decrease heatmap\n",
    "df_filtered5 = results_dtc_df[\n",
    "    (results_dtc_df['param_criterion'] == 'gini') &\n",
    "    (results_dtc_df['param_max_depth'] == 13) &\n",
    "    (results_dtc_df['param_min_samples_leaf'] == 2)\n",
    "]\n",
    "\n",
    "if not df_filtered5.empty:\n",
    "    pivot5 = df_filtered5.pivot_table(\n",
    "        values='mean_test_score',\n",
    "        index='param_max_features',\n",
    "        columns='param_min_impurity_decrease'\n",
    "    )\n",
    "    \n",
    "    if not pivot5.empty and pivot5.size > 0:\n",
    "        sns.heatmap(pivot5, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=axes[2, 0])\n",
    "        axes[2, 0].set_title('max_features vs min_impurity_decrease\\n(criterion=gini, max_depth=13, min_samples_leaf=2)')\n",
    "        axes[2, 0].set_xlabel('min_impurity_decrease')\n",
    "        axes[2, 0].set_ylabel('max_features')\n",
    "    else:\n",
    "        axes[2, 0].text(0.5, 0.5, \"No data available for\\nmax_features vs min_impurity_decrease heatmap\", \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "        axes[2, 0].set_title('max_features vs min_impurity_decrease')\n",
    "else:\n",
    "    axes[2, 0].text(0.5, 0.5, \"No data available for\\nmax_features vs min_impurity_decrease heatmap\", \n",
    "                ha='center', va='center', fontsize=12)\n",
    "    axes[2, 0].set_title('max_features vs min_impurity_decrease')\n",
    "\n",
    "# 6. Bar chart of max_features performance\n",
    "max_features_scores = results_dtc_df.groupby('param_max_features')['mean_test_score'].mean()\n",
    "sns.barplot(x=max_features_scores.index, y=max_features_scores.values, ax=axes[2, 1])\n",
    "axes[2, 1].set_title('Average Performance by max_features Value')\n",
    "axes[2, 1].set_xlabel('max_features')\n",
    "axes[2, 1].set_ylabel('Mean Test Score')\n",
    "for i, v in enumerate(max_features_scores.values):\n",
    "    axes[2, 1].text(i, v + 0.01, f\"{v:.3f}\", ha='center')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve for the optimized DTC model\n",
    "plot_roc_curve(y_vald, dtc_best_model, \"DTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tree as text\n",
    "print(export_text(dtc_best_model, feature_names=list(X_train_df.columns)))\n",
    "\n",
    "# Plot the tree\n",
    "plt.figure(figsize=(30, 20))\n",
    "plot_tree(dtc_best_model, feature_names=list(X_train_df.columns), filled=True, rounded=True, fontsize=10, max_depth=3)\n",
    "plt.title(\"Optiimized DTC Visualization\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the features with their importance scores\n",
    "feature_importances = pd.DataFrame({\n",
    "    \"feature\": selected_features,\n",
    "    \"importance\": dtc_best_model.feature_importances_\n",
    "})\n",
    "feature_importances = feature_importances.sort_values(\"importance\", ascending=False)\n",
    "\n",
    "# Label the bar with value\n",
    "plt.figure(figsize=(12, max(6, len(feature_importances) * 0.3)))  # dynamic height\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances)\n",
    "for index, value in enumerate(feature_importances['importance']):\n",
    "    plt.text(value, index, str(round(value, 6)))\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.title(\"Feature Importance (Optimized DTC)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices_side_by_side(y_vald, y_dtc_default_model_pred_val, y_dtc_best_pred_val, \"DTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(y_vald, y_dtc_default_model_pred_val, y_dtc_best_pred_val, \"Decision Tree Classifier\", \"DTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbour (KNN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An instance of KNN Classifier with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the K-Nearest Neighbors Classifier\n",
    "knn_default_model = KNeighborsClassifier()\n",
    "\n",
    "# Fit the K-Nearest Neighbors Classifier on the training set\n",
    "knn_default_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_knn_default_model_pred_val = knn_default_model.predict(X_vald)\n",
    "\n",
    "# Print the default parameters of the K-Nearest Neighbors Classifier\n",
    "print(\"Default parameters of K-Nearest Neighbors Classifier: \\n\", knn_default_model.get_params())\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_knn_default_model_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using GridSearchCV for KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the hyperparameter grid for the K-Nearest Neighbor Classifier\n",
    "# {'algorithm': 'auto', 'leaf_size': 30, 'metric': 'minkowski', 'metric_params': None, 'n_jobs': None, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
    "knn_param_grid = {\n",
    "    \"algorithm\": [\"auto\", \"ball_tree\", \"kd_tree\"],\n",
    "    \"leaf_size\": range(10, 50, 5),\n",
    "    \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "    \"n_neighbors\": range(3, 30, 2),\n",
    "    \"weights\": [\"uniform\", \"distance\"],\n",
    "}\n",
    "\n",
    "# Initialize the base K-Nearest Neighbors Classifier\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "knn_grid_search = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=knn_param_grid,\n",
    "    scoring=\"recall\", # Choose to refit the base model based on recall score\n",
    "    cv=cv, # Repeated Stratified K-Fold Cross Validation\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    error_score=0,\n",
    ")\n",
    "\n",
    "# Fit the grid search to training data\n",
    "knn_grid_results = knn_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output best parameters and score\n",
    "print(f\"✅ Best hyperparameters found: {knn_grid_results.best_params_}\")\n",
    "print(f\"✅ Best cross-validation score: {knn_grid_results.best_score_:.6f}\")\n",
    "\n",
    "# Get the best model\n",
    "knn_best_model = knn_grid_results.best_estimator_\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_knn_best_pred_val = knn_best_model.predict(X_vald)\n",
    "\n",
    "# Print the evaluation metrics for the validation set\n",
    "evaluate_validation_set(y_vald, y_knn_best_pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the results of the GridSearchCV into a DataFrame\n",
    "# Sort the DataFrame by the mean test score in descending order\n",
    "# Display the top 10 configurations with the highest mean test scores\n",
    "results_knn_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "results_knn_df.sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'], \n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_knn_df[['params', 'mean_test_score', 'rank_test_score']].sort_values(\n",
    "    by=['rank_test_score', 'mean_test_score'],\n",
    "    ascending=[True, False]\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive visualization with multiple heatmaps of KNN hyperparameters tuning results\n",
    "results_knn_df = pd.DataFrame(knn_grid_search.cv_results_)\n",
    "\n",
    "# Set up a 2x2 grid of subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "fig.suptitle('KNN Hyperparameter Tuning Results', fontsize=16)\n",
    "\n",
    "# 1. n_neighbors vs leaf_size (for auto algorithm, euclidean metric, distance weights)\n",
    "df_filtered1 = results_knn_df[\n",
    "    (results_knn_df['param_algorithm'] == 'auto') &\n",
    "    (results_knn_df['param_metric'] == 'euclidean') &\n",
    "    (results_knn_df['param_weights'] == 'distance')\n",
    "]\n",
    "pivot1 = df_filtered1.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_n_neighbors',\n",
    "    columns='param_leaf_size'\n",
    ")\n",
    "sns.heatmap(pivot1, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=axes[0, 0])\n",
    "axes[0, 0].set_title('n_neighbors vs leaf_size\\n(algorithm=auto, metric=euclidean, weights=distance)')\n",
    "axes[0, 0].set_xlabel('leaf_size')\n",
    "axes[0, 0].set_ylabel('n_neighbors')\n",
    "\n",
    "# 2. n_neighbors vs metric (for auto algorithm, leaf_size=30, distance weights)\n",
    "df_filtered2 = results_knn_df[\n",
    "    (results_knn_df['param_algorithm'] == 'auto') &\n",
    "    (results_knn_df['param_leaf_size'] == 30) &\n",
    "    (results_knn_df['param_weights'] == 'distance')\n",
    "]\n",
    "pivot2 = df_filtered2.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_n_neighbors',\n",
    "    columns='param_metric'\n",
    ")\n",
    "sns.heatmap(pivot2, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('n_neighbors vs metric\\n(algorithm=auto, leaf_size=30, weights=distance)')\n",
    "axes[0, 1].set_xlabel('metric')\n",
    "axes[0, 1].set_ylabel('n_neighbors')\n",
    "\n",
    "# 3. n_neighbors vs algorithm (for euclidean metric, leaf_size=30, distance weights)\n",
    "df_filtered3 = results_knn_df[\n",
    "    (results_knn_df['param_metric'] == 'euclidean') &\n",
    "    (results_knn_df['param_leaf_size'] == 30) &\n",
    "    (results_knn_df['param_weights'] == 'distance')\n",
    "]\n",
    "pivot3 = df_filtered3.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_n_neighbors',\n",
    "    columns='param_algorithm'\n",
    ")\n",
    "sns.heatmap(pivot3, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('n_neighbors vs algorithm\\n(metric=euclidean, leaf_size=30, weights=distance)')\n",
    "axes[1, 0].set_xlabel('algorithm')\n",
    "axes[1, 0].set_ylabel('n_neighbors')\n",
    "\n",
    "# 4. n_neighbors vs weights (for auto algorithm, euclidean metric, leaf_size=30)\n",
    "df_filtered4 = results_knn_df[\n",
    "    (results_knn_df['param_algorithm'] == 'auto') &\n",
    "    (results_knn_df['param_metric'] == 'euclidean') &\n",
    "    (results_knn_df['param_leaf_size'] == 30)\n",
    "]\n",
    "pivot4 = df_filtered4.pivot_table(\n",
    "    values='mean_test_score',\n",
    "    index='param_n_neighbors',\n",
    "    columns='param_weights'\n",
    ")\n",
    "sns.heatmap(pivot4, annot=True, fmt=\".3f\", cmap='YlGnBu', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('n_neighbors vs weights\\n(algorithm=auto, metric=euclidean, leaf_size=30)')\n",
    "axes[1, 1].set_xlabel('weights')\n",
    "axes[1, 1].set_ylabel('n_neighbors')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.97])  # Adjust layout to make room for suptitle\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** transforms the original features into a new set of uncorrelated components, which are linear combinations of the original variables. These components are ordered based on the amount of variance they capture from the data. PCA reduces dimensionality while preserving as much of the underlying data structure as possible.\n",
    "\n",
    "In this case, PCA is applied to reduce the original eight-dimensional feature space to two principal components, specifically for visualization and plotting purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to validation features\n",
    "# Reduced to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the validation set\n",
    "X_pca = pca.fit_transform(X_vald)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true labels in a separate graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y_vald, palette='Set1', s=40)\n",
    "plt.title(\"True Labels Scatter Plot (PCA-reduced 2D)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"True Class\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot side-by-side subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharex=True, sharey=True)\n",
    "\n",
    "# Default KNN\n",
    "sns.scatterplot(ax=axes[0], x=X_pca[:, 0], y=X_pca[:, 1], hue=y_knn_default_model_pred_val, palette='Set1', s=40)\n",
    "axes[0].set_title(\"Default KNN Prediction Scatter Plot (PCA-reduced 2D)\")\n",
    "axes[0].set_xlabel(\"Principal Component 1\")\n",
    "axes[0].set_ylabel(\"Principal Component 2\")\n",
    "axes[0].legend(title=\"Predicted Class\")\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Optimized KNN\n",
    "sns.scatterplot(ax=axes[1], x=X_pca[:, 0], y=X_pca[:, 1], hue=y_knn_best_pred_val, palette='Set1', s=40)\n",
    "axes[1].set_title(\"Optimized KNN Prediction Scatter Plot (PCA-reduced 2D)\")\n",
    "axes[1].set_xlabel(\"Principal Component 1\")\n",
    "axes[1].set_ylabel(\"Principal Component 2\")\n",
    "axes[1].legend(title=\"Predicted Class\")\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare predictions\n",
    "y_pred_default = knn_default_model.predict(X_vald)\n",
    "y_pred_optimized = knn_best_model.predict(X_vald)\n",
    "\n",
    "# Identify points where predictions differ\n",
    "diff_mask = y_pred_default != y_pred_optimized\n",
    "diff_labels = pd.Series(diff_mask).map({True: 'Yes', False: 'No'})\n",
    "\n",
    "# PCA transform\n",
    "X_pca = PCA(n_components=2).fit_transform(X_vald)\n",
    "\n",
    "# Plot only the changed predictions\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=diff_labels, palette='deep', s=40)\n",
    "plt.title(\"Points with Different Predictions (Default vs Optimized KNN)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Prediction Changed\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = knn_best_model.predict_proba(X_vald)[:, 1]\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=probs, palette='coolwarm', s=40)\n",
    "plt.title(\"Probability of Class 1 (Optimized KNN)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices_side_by_side(y_vald, y_knn_default_model_pred_val, y_knn_best_pred_val, \"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_comparison(y_vald, y_knn_default_model_pred_val, y_knn_best_pred_val, \"K-Nearest Neighbors Classifier\", \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the models\n",
    "Perform a comparison between the predictive models. <br>\n",
    "Report the accuracy, recall, precision and F1-score measures as well as the confusion matrix if it is a classification problem. <br>\n",
    "Report the R2 score, mean squared error and mean absolute error if it is a regression problem.\n",
    "______________________________________________________________________________________\n",
    "Description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a model using multiple metrics (weighted and macro)\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Weighted Precision': precision_score(y_true, y_pred, average='weighted'),\n",
    "        'Weighted Recall': recall_score(y_true, y_pred, average='weighted'),\n",
    "        'Weighted F1 Score': f1_score(y_true, y_pred, average='weighted'),\n",
    "        'Macro Precision': precision_score(y_true, y_pred, average='macro'),\n",
    "        'Macro Recall': recall_score(y_true, y_pred, average='macro'),\n",
    "        'Macro F1 Score': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(y_true, model1_pred, model2_pred, model3_pred, \n",
    "                   model1_name, model2_name, model3_name):\n",
    "    \"\"\"\n",
    "    Compare three models using various metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics for each model\n",
    "    metrics1 = evaluate_model(y_true, model1_pred, model1_name)\n",
    "    metrics2 = evaluate_model(y_true, model2_pred, model2_name)\n",
    "    metrics3 = evaluate_model(y_true, model3_pred, model3_name)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame([metrics1, metrics2, metrics3])\n",
    "\n",
    "    cm_model1 = confusion_matrix(y_true, model1_pred)\n",
    "    cm_model2 = confusion_matrix(y_true, model2_pred)\n",
    "    cm_model3 = confusion_matrix(y_true, model3_pred)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "    # Confusion matrix for model 1\n",
    "    sns.heatmap(cm_model1, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title(f'Confusion Matrix - Optimized ' + model1_name)\n",
    "    axes[0].set_xlabel('Predicted Label')\n",
    "    axes[0].set_ylabel('True Label')\n",
    "    \n",
    "    # Confusion matrix for model 2\n",
    "    sns.heatmap(cm_model2, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "    axes[1].set_title(f'Confusion Matrix - Optimized ' + model2_name)\n",
    "    axes[1].set_xlabel('Predicted Label')\n",
    "    axes[1].set_ylabel('True Label')\n",
    "\n",
    "    # Confusion matrix for model 3\n",
    "    sns.heatmap(cm_model3, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
    "    axes[2].set_title(f'Confusion Matrix - Optimized ' + model3_name)\n",
    "    axes[2].set_xlabel('Predicted Label')\n",
    "    axes[2].set_ylabel('True Label')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot comparison bar chart - Accuracy, Weighted Precision, Weighted Recall, Weighted F1 Score\n",
    "    metrics_to_plot = ['Accuracy', 'Weighted Precision', 'Weighted Recall', 'Weighted F1 Score']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = plt.bar(x - width, comparison_df[metrics_to_plot].iloc[0], width, label=model1_name)\n",
    "    bars2 = plt.bar(x, comparison_df[metrics_to_plot].iloc[1], width, label=model2_name)\n",
    "    bars3 = plt.bar(x + width, comparison_df[metrics_to_plot].iloc[2], width, label=model3_name)\n",
    "\n",
    "    # Annotate bars with their values\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Comparison (Weighted Avg)')\n",
    "    plt.xticks(x, metrics_to_plot)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot comparison bar chart - Accuracy, Macro Precision, Macro Recall, Macro F1 Score\n",
    "    metrics_to_plot = ['Accuracy', 'Macro Precision', 'Macro Recall', 'Macro F1 Score']\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = plt.bar(x - width, comparison_df[metrics_to_plot].iloc[0], width, label=model1_name)\n",
    "    bars2 = plt.bar(x, comparison_df[metrics_to_plot].iloc[1], width, label=model2_name)\n",
    "    bars3 = plt.bar(x + width, comparison_df[metrics_to_plot].iloc[2], width, label=model3_name)\n",
    "\n",
    "    # Annotate bars with their values\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    for bar in bars3:\n",
    "        height = bar.get_height()\n",
    "        plt.annotate(f'{height:.4f}', xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3), textcoords=\"offset points\", ha='center', va='bottom')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Model Comparison (Macro Avg)')\n",
    "    plt.xticks(x, metrics_to_plot)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed classification reports\n",
    "    print(f\"\\nClassification Report for {model1_name}:\")\n",
    "    print(classification_report(y_true, model1_pred))\n",
    "    print(f\"\\nClassification Report for {model2_name}:\")\n",
    "    print(classification_report(y_true, model2_pred))\n",
    "    print(f\"\\nClassification Report for {model3_name}:\")\n",
    "    print(classification_report(y_true, model3_pred))\n",
    "    \n",
    "    return comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_svm = svm_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier (DTC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dtc = dtc_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### K-Nearest Neighbour (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn = knn_best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_models(\n",
    "    y_test,\n",
    "    y_pred_svm,\n",
    "    y_pred_dtc,\n",
    "    y_pred_knn,\n",
    "    model1_name=\"SVM\",\n",
    "    model2_name=\"DTC\",\n",
    "    model3_name=\"KNN\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed results\n",
    "print(\"\\nDetailed Model Comparison:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
